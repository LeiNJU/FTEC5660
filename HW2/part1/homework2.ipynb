{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "RRbStil_qkQc",
        "kCENjOq6owDd"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Homework 2"
      ],
      "metadata": {
        "id": "tFaf7VErZ1Bf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up"
      ],
      "metadata": {
        "id": "I3g3k3W-qfAv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing packages"
      ],
      "metadata": {
        "id": "6vsESFZylO6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests PyPDF2 gdown\n",
        "!pip install 'markitdown[pdf]'\n",
        "!pip install langchain_mcp_adapters langchain_google_genai langchain-openai"
      ],
      "metadata": {
        "id": "Hrdfpmv9nMpw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ef62869-f61e-401e-f17e-f77f67eb2613"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.12/dist-packages (5.2.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2026.1.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from gdown) (4.13.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from gdown) (3.24.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from gdown) (4.67.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (2.8.3)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (4.15.0)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "Collecting markitdown[pdf]\n",
            "  Downloading markitdown-0.1.5-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from markitdown[pdf]) (4.13.5)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from markitdown[pdf]) (3.4.4)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.12/dist-packages (from markitdown[pdf]) (0.7.1)\n",
            "Collecting magika~=0.6.1 (from markitdown[pdf])\n",
            "  Downloading magika-0.6.3-py3-none-manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
            "Collecting markdownify (from markitdown[pdf])\n",
            "  Downloading markdownify-1.2.2-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from markitdown[pdf]) (2.32.4)\n",
            "Collecting pdfminer-six>=20251230 (from markitdown[pdf])\n",
            "  Downloading pdfminer_six-20260107-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting pdfplumber>=0.11.9 (from markitdown[pdf])\n",
            "  Downloading pdfplumber-0.11.9-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click>=8.1.7 in /usr/local/lib/python3.12/dist-packages (from magika~=0.6.1->markitdown[pdf]) (8.3.1)\n",
            "Collecting onnxruntime>=1.17.0 (from magika~=0.6.1->markitdown[pdf])\n",
            "  Downloading onnxruntime-1.24.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: numpy>=1.26 in /usr/local/lib/python3.12/dist-packages (from magika~=0.6.1->markitdown[pdf]) (2.0.2)\n",
            "Requirement already satisfied: python-dotenv>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from magika~=0.6.1->markitdown[pdf]) (1.2.1)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer-six>=20251230->markitdown[pdf]) (43.0.3)\n",
            "Collecting pdfminer-six>=20251230 (from markitdown[pdf])\n",
            "  Downloading pdfminer_six-20251230-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.12/dist-packages (from pdfplumber>=0.11.9->markitdown[pdf]) (11.3.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber>=0.11.9->markitdown[pdf])\n",
            "  Downloading pypdfium2-5.5.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (68 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m68.1/68.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->markitdown[pdf]) (2.8.3)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->markitdown[pdf]) (4.15.0)\n",
            "Requirement already satisfied: six<2,>=1.15 in /usr/local/lib/python3.12/dist-packages (from markdownify->markitdown[pdf]) (1.17.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->markitdown[pdf]) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->markitdown[pdf]) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->markitdown[pdf]) (2026.1.4)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=36.0.0->pdfminer-six>=20251230->markitdown[pdf]) (2.0.0)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.17.0->magika~=0.6.1->markitdown[pdf]) (25.12.19)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.17.0->magika~=0.6.1->markitdown[pdf]) (26.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.17.0->magika~=0.6.1->markitdown[pdf]) (5.29.6)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.17.0->magika~=0.6.1->markitdown[pdf]) (1.14.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer-six>=20251230->markitdown[pdf]) (3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.17.0->magika~=0.6.1->markitdown[pdf]) (1.3.0)\n",
            "Downloading magika-0.6.3-py3-none-manylinux_2_28_x86_64.whl (15.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m15.4/15.4 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfplumber-0.11.9-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer_six-20251230-py3-none-any.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m86.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading markdownify-1.2.2-py3-none-any.whl (15 kB)\n",
            "Downloading markitdown-0.1.5-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.4/63.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.24.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m75.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-5.5.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2, onnxruntime, markdownify, pdfminer-six, magika, pdfplumber, markitdown\n",
            "Successfully installed magika-0.6.3 markdownify-1.2.2 markitdown-0.1.5 onnxruntime-1.24.2 pdfminer-six-20251230 pdfplumber-0.11.9 pypdfium2-5.5.0\n",
            "Collecting langchain_mcp_adapters\n",
            "  Downloading langchain_mcp_adapters-0.2.1-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting langchain_google_genai\n",
            "  Downloading langchain_google_genai-4.2.1-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-1.1.10-py3-none-any.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain_mcp_adapters) (1.2.13)\n",
            "Requirement already satisfied: mcp>=1.9.2 in /usr/local/lib/python3.12/dist-packages (from langchain_mcp_adapters) (1.26.0)\n",
            "Requirement already satisfied: typing-extensions>=4.14.0 in /usr/local/lib/python3.12/dist-packages (from langchain_mcp_adapters) (4.15.0)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain_google_genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: google-genai<2.0.0,>=1.56.0 in /usr/local/lib/python3.12/dist-packages (from langchain_google_genai) (1.63.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain_google_genai) (2.12.3)\n",
            "Requirement already satisfied: openai<3.0.0,>=2.20.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (2.21.0)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain_google_genai) (4.12.1)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.47.0 in /usr/local/lib/python3.12/dist-packages (from google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain_google_genai) (2.47.0)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain_google_genai) (0.28.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain_google_genai) (2.32.4)\n",
            "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain_google_genai) (9.1.4)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain_google_genai) (15.0.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain_google_genai) (1.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain_google_genai) (1.3.1)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain_mcp_adapters) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain_mcp_adapters) (0.7.3)\n",
            "Requirement already satisfied: packaging>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain_mcp_adapters) (26.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain_mcp_adapters) (6.0.3)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain_mcp_adapters) (0.14.0)\n",
            "Requirement already satisfied: httpx-sse>=0.4 in /usr/local/lib/python3.12/dist-packages (from mcp>=1.9.2->langchain_mcp_adapters) (0.4.3)\n",
            "Requirement already satisfied: jsonschema>=4.20.0 in /usr/local/lib/python3.12/dist-packages (from mcp>=1.9.2->langchain_mcp_adapters) (4.26.0)\n",
            "Requirement already satisfied: pydantic-settings>=2.5.2 in /usr/local/lib/python3.12/dist-packages (from mcp>=1.9.2->langchain_mcp_adapters) (2.13.0)\n",
            "Requirement already satisfied: pyjwt>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from pyjwt[crypto]>=2.10.1->mcp>=1.9.2->langchain_mcp_adapters) (2.11.0)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.12/dist-packages (from mcp>=1.9.2->langchain_mcp_adapters) (0.0.22)\n",
            "Requirement already satisfied: sse-starlette>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from mcp>=1.9.2->langchain_mcp_adapters) (3.2.0)\n",
            "Requirement already satisfied: starlette>=0.27 in /usr/local/lib/python3.12/dist-packages (from mcp>=1.9.2->langchain_mcp_adapters) (0.52.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from mcp>=1.9.2->langchain_mcp_adapters) (0.4.2)\n",
            "Requirement already satisfied: uvicorn>=0.31.1 in /usr/local/lib/python3.12/dist-packages (from mcp>=1.9.2->langchain_mcp_adapters) (0.41.0)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=2.20.0->langchain-openai) (0.13.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=2.20.0->langchain-openai) (4.67.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->langchain_google_genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->langchain_google_genai) (2.41.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2025.11.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.56.0->langchain_google_genai) (3.11)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain_google_genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain_google_genai) (4.9.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.56.0->langchain_google_genai) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.56.0->langchain_google_genai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.56.0->langchain_google_genai) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain_mcp_adapters) (3.0.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp>=1.9.2->langchain_mcp_adapters) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp>=1.9.2->langchain_mcp_adapters) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp>=1.9.2->langchain_mcp_adapters) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp>=1.9.2->langchain_mcp_adapters) (0.30.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain_mcp_adapters) (3.11.7)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain_mcp_adapters) (1.0.0)\n",
            "Requirement already satisfied: xxhash>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain_mcp_adapters) (3.6.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain_mcp_adapters) (0.25.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings>=2.5.2->mcp>=1.9.2->langchain_mcp_adapters) (1.2.1)\n",
            "Requirement already satisfied: cryptography>=3.4.0 in /usr/local/lib/python3.12/dist-packages (from pyjwt[crypto]>=2.10.1->mcp>=1.9.2->langchain_mcp_adapters) (43.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.28.1->google-genai<2.0.0,>=1.56.0->langchain_google_genai) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.28.1->google-genai<2.0.0,>=1.56.0->langchain_google_genai) (2.5.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn>=0.31.1->mcp>=1.9.2->langchain_mcp_adapters) (8.3.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=3.4.0->pyjwt[crypto]>=2.10.1->mcp>=1.9.2->langchain_mcp_adapters) (2.0.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain_google_genai) (0.6.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=3.4.0->pyjwt[crypto]>=2.10.1->mcp>=1.9.2->langchain_mcp_adapters) (3.0)\n",
            "Downloading langchain_mcp_adapters-0.2.1-py3-none-any.whl (22 kB)\n",
            "Downloading langchain_google_genai-4.2.1-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-1.1.10-py3-none-any.whl (87 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Installing collected packages: filetype, langchain-openai, langchain_mcp_adapters, langchain_google_genai\n",
            "Successfully installed filetype-1.2.0 langchain-openai-1.1.10 langchain_google_genai-4.2.1 langchain_mcp_adapters-0.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup your API key\n",
        "\n",
        "To run the following cell, your API key must be stored it in a Colab Secret named `VERTEX_API_KEY`.\n",
        "\n",
        "\n",
        "1.   Look for the key icon on the left panel of your colab.\n",
        "2.   Under `Name`, create `VERTEX_API_KEY`.\n",
        "3. Copy your key to `Value`.\n",
        "\n",
        "If you cannot use VERTEX_API_KEY, you can use deepseek models via `DEEPSEEK_API_KEY`. It does not affect your score.\n",
        "\n"
      ],
      "metadata": {
        "id": "BUav-7KdaY_W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "# GEMINI_VERTEX_API_KEY = userdata.get('VERTEX_API_KEY')\n",
        "DEEPSEEK_API_KEY = userdata.get('DEEPSEEK_API_KEY')"
      ],
      "metadata": {
        "id": "ueILmCPHci9v"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download sample CVs"
      ],
      "metadata": {
        "id": "RRbStil_qkQc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading sample_cv.pdf\n",
        "The codes below download the sample CV\n"
      ],
      "metadata": {
        "id": "kCENjOq6owDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gdown\n",
        "\n",
        "folder_id = \"1adYKq7gSSczFP3iikfA8Er-HSZP6VM7D\"\n",
        "folder_url = f\"https://drive.google.com/drive/folders/{folder_id}\"\n",
        "\n",
        "output_dir = \"downloaded_cvs\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "gdown.download_folder(\n",
        "    url=folder_url,\n",
        "    output=output_dir,\n",
        "    quiet=False,\n",
        "    use_cookies=False\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kCCp8DwPF4L",
        "outputId": "975aa517-992d-44b9-cfbe-50e6005f280a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving folder contents\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing file 1NR1RUKx4GyM7QOBxKXkfh4e8jUkxFCsp CV_1.pdf\n",
            "Processing file 16lrd-uO8AAnCnv7UG9Rs_Nk6SUu0Iwbs CV_2.pdf\n",
            "Processing file 15hVEuBan_EKhEty2aZBd6rcpDpP4o7Vr CV_3.pdf\n",
            "Processing file 1Y2w_mAUEhg4vZBdvvR-0n3Jf2mKuGDRk CV_4.pdf\n",
            "Processing file 1PLwkva-pdua6ZVvmLg9mxHeljq9D8C_C CV_5.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving folder contents completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1NR1RUKx4GyM7QOBxKXkfh4e8jUkxFCsp\n",
            "To: /content/downloaded_cvs/CV_1.pdf\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 147k/147k [00:00<00:00, 35.4MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=16lrd-uO8AAnCnv7UG9Rs_Nk6SUu0Iwbs\n",
            "To: /content/downloaded_cvs/CV_2.pdf\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75.1k/75.1k [00:00<00:00, 12.5MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=15hVEuBan_EKhEty2aZBd6rcpDpP4o7Vr\n",
            "To: /content/downloaded_cvs/CV_3.pdf\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 72.0k/72.0k [00:00<00:00, 18.2MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Y2w_mAUEhg4vZBdvvR-0n3Jf2mKuGDRk\n",
            "To: /content/downloaded_cvs/CV_4.pdf\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 73.3k/73.3k [00:00<00:00, 66.7MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1PLwkva-pdua6ZVvmLg9mxHeljq9D8C_C\n",
            "To: /content/downloaded_cvs/CV_5.pdf\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97.9k/97.9k [00:00<00:00, 11.3MB/s]\n",
            "Download completed\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['downloaded_cvs/CV_1.pdf',\n",
              " 'downloaded_cvs/CV_2.pdf',\n",
              " 'downloaded_cvs/CV_3.pdf',\n",
              " 'downloaded_cvs/CV_4.pdf',\n",
              " 'downloaded_cvs/CV_5.pdf']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "#  Load and display all CV PDFs in order\n",
        "# =====================================================\n",
        "import os\n",
        "from markitdown import MarkItDown\n",
        "\n",
        "cv_dir = \"downloaded_cvs\"\n",
        "\n",
        "# Initialize MarkItDown\n",
        "md = MarkItDown(enable_plugins=False)\n",
        "\n",
        "# Collect and sort PDFs numerically\n",
        "pdf_files = sorted(\n",
        "    [f for f in os.listdir(cv_dir) if f.lower().endswith(\".pdf\")],\n",
        "    key=lambda x: int(\"\".join(filter(str.isdigit, x)))  # CV_1.pdf â†’ 1\n",
        ")\n",
        "\n",
        "all_cvs = []\n",
        "\n",
        "for pdf_name in pdf_files:\n",
        "    pdf_path = os.path.join(cv_dir, pdf_name)\n",
        "    result = md.convert(pdf_path)\n",
        "\n",
        "    all_cvs.append({\n",
        "        \"file\": pdf_name,\n",
        "        \"text\": result.text_content\n",
        "    })\n",
        "\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"ğŸ“„ {pdf_name}\")\n",
        "    print(\"=\" * 80)\n",
        "    print(result.text_content)\n",
        "    print(\"\\n\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2akmVn9LODIu",
        "outputId": "97d5b8e1-f08a-4b00-db60-c9bf59294347"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "ğŸ“„ CV_1.pdf\n",
            "================================================================================\n",
            "|     |     |     |     | John         |           | Smith        |                   |     |     |\n",
            "| --- | --- | --- | --- | ------------ | --------- | ------------ | ----------------- | --- | --- |\n",
            "|     |     |     |     | Marketing    |           | Professional |                   |     |     |\n",
            "|     |     |     |     | + Singapore, | Singapore |              | (cid:209) Kowloon |     |     |\n",
            "Experience\n",
            "|                |                  |     |          |                     |              |            |     | 2020 â€“ | Present |\n",
            "| -------------- | ---------------- | --- | -------- | ------------------- | ------------ | ---------- | --- | ------ | ------- |\n",
            "| Engineer,      | ByteDance        |     |          |                     |              |            |     |        |         |\n",
            "| â€¢ Worked       | in a fast-paced, |     | global   | technology          | environment. |            |     |        |         |\n",
            "| â€¢ Collaborated | across           |     | teams to | support large-scale |              | platforms. |     |        |         |\n",
            "â€¢ Applied analytical and problem-solving skills in production systems.\n",
            "Education\n",
            "| McGill   | University |       |              |     |     |     |     | Graduated | 2009 |\n",
            "| -------- | ---------- | ----- | ------------ | --- | --- | --- | --- | --------- | ---- |\n",
            "| Bachelor | of Science | (BSc) | in Marketing |     |     |     |     |           |      |\n",
            "Skills\n",
            "| Content | Creation | SEO | Social | Media |     |     |     |     |     |\n",
            "| ------- | -------- | --- | ------ | ----- | --- | --- | --- | --- | --- |\n",
            "1\n",
            "\n",
            "\n",
            "\n",
            "================================================================================\n",
            "ğŸ“„ CV_2.pdf\n",
            "================================================================================\n",
            "| Minh | Pham |     |     |     |     |     |\n",
            "| ---- | ---- | --- | --- | --- | --- | --- |\n",
            "Design Professional\n",
            "| Beijing,     | China | Hong     | Kong     |               |        |              |                |\n",
            "| ------------ | ---------------- | -------- | ------------- | ------ | ------------ | -------------- |\n",
            "| Professional | Experience       |          |               |        |              |                |\n",
            "| Manager,     | BCG              |          |               |        |              | 2022 â€“ Present |\n",
            "| â€¢ Led        | cross-functional | teams on | client-facing | design | initiatives. |                |\n",
            "â€¢ Managed project timelines, deliverables, and stakeholder communication.\n",
            "| â€¢ Applied | design thinking | to business | and | strategy | problems. |             |\n",
            "| --------- | --------------- | ----------- | --- | -------- | --------- | ----------- |\n",
            "| Analyst,  | Tencent         |             |     |          |           | 2013 â€“ 2017 |\n",
            "â€¢ Conducted market and product analysis to support decision-making.\n",
            "| â€¢ Collaborated | with    | design and   | engineering | teams.      |     |     |\n",
            "| -------------- | ------- | ------------ | ----------- | ----------- | --- | --- |\n",
            "| â€¢ Produced     | reports | and insights | for senior  | leadership. |     |     |\n",
            "Education\n",
            "| BSc in         | Design  |      |     |     |     | 2011 |\n",
            "| -------------- | ------- | ---- | --- | --- | --- | ---- |\n",
            "| The University | of Hong | Kong |     |     |     |      |\n",
            "Skills\n",
            "| â€¢ UI/UX | Design |     |     |     |     |     |\n",
            "| ------- | ------ | --- | --- | --- | --- | --- |\n",
            "â€¢ Prototyping\n",
            "| â€¢ Graphic | Design |     |     |     |     |     |\n",
            "| --------- | ------ | --- | --- | --- | --- | --- |\n",
            "1\n",
            "\n",
            "\n",
            "\n",
            "================================================================================\n",
            "ğŸ“„ CV_3.pdf\n",
            "================================================================================\n",
            "| Wei Zhang    |              |           |     |     |     | Munich, Germany   |\n",
            "| ------------ | ------------ | --------- | --- | --- | --- | ----------------- |\n",
            "| Consulting   | Professional |           |     |     |     | Sydney (Hometown) |\n",
            "| Professional | Experience   |           |     |     |     |                   |\n",
            "| 2013         | â€“ Present    | Engineer, | PwC |     |     |                   |\n",
            "â€¢ Supportedconsultingengagementsacrossmultipleclient\n",
            "projects.\n",
            "|     |     | â€¢ Performed | data analysis | to inform | strategic recommen- |     |\n",
            "| --- | --- | ----------- | ------------- | --------- | ------------------- | --- |\n",
            "dations.\n",
            "|     |     | â€¢ Collaborated  | with         | cross-functional | teams in | a profes- |\n",
            "| --- | --- | --------------- | ------------ | ---------------- | -------- | --------- |\n",
            "|     |     | sional services | environment. |                  |          |           |\n",
            "Education\n",
            "| 2015 |     | BSc in Consulting |          |     |     |     |\n",
            "| ---- | --- | ----------------- | -------- | --- | --- | --- |\n",
            "|      |     | University        | of Tokyo |     |     |     |\n",
            "Skills\n",
            "| Analytical |     |     | Data Analysis,       | Problem | Solving |     |\n",
            "| ---------- | --- | --- | -------------------- | ------- | ------- | --- |\n",
            "| Business   |     |     | Strategy, PowerPoint |         |         |     |\n",
            "1\n",
            "\n",
            "\n",
            "\n",
            "================================================================================\n",
            "ğŸ“„ CV_4.pdf\n",
            "================================================================================\n",
            "| Rahul | Sharma |     |     |     |     |     |     |     |     |\n",
            "| ----- | ------ | --- | --- | --- | --- | --- | --- | --- | --- |\n",
            "Legal Professional\n",
            "| Singapore    | (Hometown) | | Singapore              |           | / Philippines |             |        |             |     |         |\n",
            "| ------------ | ---------- | ------------------------ | --------- | ------------- | ----------- | ------ | ----------- | --- | ------- |\n",
            "| Professional |            | Experience               |           |               |             |        |             |     |         |\n",
            "| 2021         | â€“ 2027     | Senior                   | Engineer, | Microsoft     |             |        |             |     |         |\n",
            "|              |            | â€¢ Led compliance-focused |           |               | initiatives | within | large-scale |     | techni- |\n",
            "cal teams.\n",
            "â€¢ Advisedonregulatory,legal,andriskconsiderationsforcom-\n",
            "plex systems.\n",
            "|     |     | â€¢ Worked | at the | intersection |     | of law, | technology, | and | gover- |\n",
            "| --- | --- | -------- | ------ | ------------ | --- | ------- | ----------- | --- | ------ |\n",
            "nance.\n",
            "| 2020 | â€“ 2023 | Consultant, | StartupXYZ |               |     |            |                 |     |      |\n",
            "| ---- | ------ | ----------- | ---------- | ------------- | --- | ---------- | --------------- | --- | ---- |\n",
            "|      |        | â€¢ Provided  | legal      | and strategic |     | consulting | for early-stage |     | com- |\n",
            "panies.\n",
            "|     |     | â€¢ Supported | contract | review, |     | compliance, | and operational |     | risk |\n",
            "| --- | --- | ----------- | -------- | ------- | --- | ----------- | --------------- | --- | ---- |\n",
            "management.\n",
            "|     |     | â€¢ Engaged | with | cross-functional |     | and | international | stakehold- |     |\n",
            "| --- | --- | --------- | ---- | ---------------- | --- | --- | ------------- | ---------- | --- |\n",
            "ers.\n",
            "Education\n",
            "2021\n",
            "|     |     | PhD in   | Legal      | Studies |     |     |     |     |     |\n",
            "| --- | --- | -------- | ---------- | ------- | --- | --- | --- | --- | --- |\n",
            "|     |     | Tsinghua | University |         |     |     |     |     |     |\n",
            "Skills\n",
            "|     |     | Compliance,   | Litigation, |           | Contract | Review    |     |     |     |\n",
            "| --- | --- | ------------- | ----------- | --------- | -------- | --------- | --- | --- | --- |\n",
            "|     |     | Web3, Machine |             | Learning, | Quantum  | Computing |     |     |     |\n",
            "1\n",
            "\n",
            "\n",
            "\n",
            "================================================================================\n",
            "ğŸ“„ CV_5.pdf\n",
            "================================================================================\n",
            "| Rahul | Sharma |     |     |     |     |     |     |\n",
            "| ----- | ------ | --- | --- | --- | --- | --- | --- |\n",
            "AI Professional\n",
            "| London     | | Hong Kong | | Singapore | (Hometown) |              |               |                |               |\n",
            "| ---------- | ----------- | ----------- | ---------- | ------------ | ------------- | -------------- | ------------- |\n",
            "| Core       | Skills      |             |            | Professional | Experience    |                |               |\n",
            "| Machine    | Learning    | & AI        |            | Senior       | Engineer      |                |               |\n",
            "|            |             |             |            | EY           |               |                | Current       |\n",
            "| â€¢ Advanced | AI Systems  |             |            |              |               |                |               |\n",
            "|            |             |             |            | â€¢ Designed   | and evaluated | AI-driven      | solutions for |\n",
            "|            |             |             |            | enterprise   | clients.      |                |               |\n",
            "| â€¢ Machine  | Learning    | (ML)        |            |              |               |                |               |\n",
            "|            |             |             |            | â€¢ Applied    | ML techniques | to large-scale | business      |\n",
            "| â€¢ Natural  | Language    | Processing  | (NLP)      | problems.    |               |                |               |\n",
            "Consultant\n",
            "| Frameworks   | &   | Tools |     |             |             |          |             |\n",
            "| ------------ | --- | ----- | --- | ----------- | ----------- | -------- | ----------- |\n",
            "|              |     |       |     | StartupXYZ  |             |          | 2019 â€“ 2021 |\n",
            "| â€¢ TensorFlow |     |       |     | â€¢ Provided  | AI and data | strategy | advisory to |\n",
            "|              |     |       |     | early-stage | companies.  |          |             |\n",
            "â€¢ PyTorch\n",
            "Senior Analyst\n",
            "|     |     |     |     | DataForge |     | 2016 | â€“ Present |\n",
            "| --- | --- | --- | --- | --------- | --- | ---- | --------- |\n",
            "â€¢ Python\n",
            "|     |     |     |     | â€¢ Conducted | advanced | data analysis | and model |\n",
            "| --- | --- | --- | --- | ----------- | -------- | ------------- | --------- |\n",
            "evaluation.\n",
            "Lead Scientist\n",
            "Education\n",
            "|     |     |     |     | UrbanFlow |     |     | 2010 â€“ 2017 |\n",
            "| --- | --- | --- | --- | --------- | --- | --- | ----------- |\n",
            "PhD in Artificial Intelligence â€¢ Led research initiatives in applied AI systems.\n",
            "| University | of Tokyo |     |     |            |                    |                |     |\n",
            "| ---------- | -------- | --- | --- | ---------- | ------------------ | -------------- | --- |\n",
            "| 2012       |          |     |     | â€¢ Mentored | junior researchers | and engineers. |     |\n",
            "1\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Connect to our MCP server\n",
        "\n",
        "Documentation about MCP: https://modelcontextprotocol.io/docs/getting-started/intro.\n",
        "\n",
        "Using MCP servers in Langchain https://docs.langchain.com/oss/python/langchain/mcp."
      ],
      "metadata": {
        "id": "VA2GvPWTQFt9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check which tools that the MCP server provide"
      ],
      "metadata": {
        "id": "5mbkH9xHXfmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import json\n",
        "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
        "\n",
        "client = MultiServerMCPClient({\n",
        "    \"social_graph\": {\n",
        "        \"transport\": \"http\",\n",
        "        \"url\": \"https://ftec5660.ngrok.app/mcp\",\n",
        "        \"headers\": {\"ngrok-skip-browser-warning\": \"true\"}\n",
        "    }\n",
        "})\n",
        "\n",
        "mcp_tools = await client.get_tools()\n",
        "for tool in mcp_tools:\n",
        "    print(tool.name)\n",
        "    print(tool.description)\n",
        "    print(tool.args)\n",
        "    print(\"\\n\\n------------------------------------------------------\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6h0311KbN9A3",
        "outputId": "6318e08f-9fa9-47a7-e7d8-1caf96ad6379"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "search_facebook_users\n",
            "Search for Facebook users by display name (supports partial and fuzzy matching).\n",
            "\n",
            "Args:\n",
            "    q: Search query string (case-insensitive, matches any part of display name)\n",
            "       Examples: \"John\", \"john smith\", \"Smith\"\n",
            "    limit: Maximum number of results to return (default: 20, max: 20)\n",
            "    fuzzy: Enable fuzzy matching if exact search returns no results (default: True)\n",
            "\n",
            "Returns:\n",
            "    List of user dictionaries, each containing:\n",
            "    - id (int): Unique Facebook user ID for use with get_facebook_profile()\n",
            "    - display_name (str): User's Facebook display name (may differ from legal name)\n",
            "    - city (str): Current city of residence\n",
            "    - country (str): Country of residence\n",
            "    - match_type (str): \"exact\" or \"fuzzy\" (indicates search method used)\n",
            "    \n",
            "    Returns empty list [] if no matches found.\n",
            "\n",
            "Example:\n",
            "    search_facebook_users(\"Alex Chan\", limit=5)\n",
            "    â†’ [{\"id\": 123, \"display_name\": \"Alex Chan\", \"city\": \"Hong Kong\", \"country\": \"Hong Kong\", \"match_type\": \"exact\"}]\n",
            "    \n",
            "    search_facebook_users(\"Alx Chn\", limit=5)  # Typo - uses fuzzy matching\n",
            "    â†’ [{\"id\": 123, \"display_name\": \"Alex Chan\", \"city\": \"Hong Kong\", \"country\": \"Hong Kong\", \"match_type\": \"fuzzy\"}]\n",
            "\n",
            "Use case:\n",
            "    First step in CV verification - find candidate's Facebook profile to cross-check\n",
            "    personal information, location, and social connections. Handles typos and variations.\n",
            "{'q': {'type': 'string'}, 'limit': {'default': 20, 'type': 'integer'}, 'fuzzy': {'default': True, 'type': 'boolean'}}\n",
            "\n",
            "\n",
            "------------------------------------------------------\n",
            "\n",
            "\n",
            "get_facebook_profile\n",
            "Retrieve complete Facebook profile including personal info, bio, relationships, and activity.\n",
            "\n",
            "Args:\n",
            "    user_id: Facebook user ID obtained from search_facebook_users()\n",
            "\n",
            "Returns:\n",
            "    Dictionary containing:\n",
            "    - id (int): Facebook user ID\n",
            "    - display_name (str): Public display name (may be nickname)\n",
            "    - original_name (str): Original/legal name from LinkedIn\n",
            "    - city (str): Current city\n",
            "    - country (str): Current country\n",
            "    - hometown (str|None): City/region where user grew up\n",
            "    - bio (str): Personal biography/interests\n",
            "    - status (str|None): Relationship status (Single, Married, etc.)\n",
            "    - education (str|None): Highest education level\n",
            "    - current_job (str|None): Current job title\n",
            "    - current_company (str|None): Current employer\n",
            "    - interests (str): Comma-separated hobbies/interests\n",
            "    - friends (List[int]): List of friend user IDs\n",
            "    - posts (List[dict]): Recent posts with id and content\n",
            "    \n",
            "    Returns {\"error\": \"User not found\"} if user_id is invalid.\n",
            "\n",
            "Example:\n",
            "    get_facebook_profile(123)\n",
            "    â†’ {\n",
            "        \"id\": 123,\n",
            "        \"display_name\": \"Sam Chan\",\n",
            "        \"original_name\": \"Alex Chan\",\n",
            "        \"city\": \"Hong Kong\",\n",
            "        \"hometown\": \"Kowloon\",\n",
            "        \"bio\": \"Software professional | Photography enthusiast\",\n",
            "        \"status\": \"Married\",\n",
            "        \"current_job\": \"Senior Engineer\",\n",
            "        \"current_company\": \"Google\",\n",
            "        \"friends\": [124, 125, 126],\n",
            "        \"posts\": [{\"id\": 1, \"content\": \"Excited to announce...\"}]\n",
            "    }\n",
            "\n",
            "Use case:\n",
            "    Verify candidate's personal details, check for name discrepancies,\n",
            "    validate current employment, and assess social connections.\n",
            "{'user_id': {'type': 'integer'}}\n",
            "\n",
            "\n",
            "------------------------------------------------------\n",
            "\n",
            "\n",
            "get_facebook_mutual_friends\n",
            "Find mutual friends between two Facebook users (useful for verifying social connections).\n",
            "\n",
            "Args:\n",
            "    user_id_1: First Facebook user ID\n",
            "    user_id_2: Second Facebook user ID\n",
            "\n",
            "Returns:\n",
            "    Dictionary containing:\n",
            "    - user_1_id (int): First user's ID\n",
            "    - user_2_id (int): Second user's ID\n",
            "    - mutual_friends (List[int]): List of shared friend IDs\n",
            "    - mutual_count (int): Number of mutual friends\n",
            "    \n",
            "    Returns {\"error\": \"...\"} if either user not found.\n",
            "\n",
            "Example:\n",
            "    get_facebook_mutual_friends(123, 456)\n",
            "    â†’ {\"user_1_id\": 123, \"user_2_id\": 456, \"mutual_friends\": [789, 790], \"mutual_count\": 2}\n",
            "\n",
            "Use case:\n",
            "    Verify professional or personal relationships claimed in CV/references.\n",
            "{'user_id_1': {'type': 'integer'}, 'user_id_2': {'type': 'integer'}}\n",
            "\n",
            "\n",
            "------------------------------------------------------\n",
            "\n",
            "\n",
            "search_linkedin_people\n",
            "Search LinkedIn profiles by name, headline, skills, or keywords with optional filters.\n",
            "\n",
            "Args:\n",
            "    q: Search query (matches name, headline, summary, or skill names)\n",
            "       Examples: \"software engineer\", \"Python\", \"data scientist\", \"Alex Chan\"\n",
            "    location: Filter by location (optional, case-insensitive, matches city OR country)\n",
            "              Examples: \"Hong Kong\", \"Singapore\", \"China\", \"USA\", \"New York\"\n",
            "    industry: Filter by industry (optional, case-insensitive)\n",
            "              Examples: \"Software\", \"Finance\", \"AI\", \"Consulting\"\n",
            "    limit: Maximum results to return (default: 20, max: 20)\n",
            "    fuzzy: Enable fuzzy matching if exact search returns no results (default: True)\n",
            "\n",
            "Returns:\n",
            "    List of profile dictionaries, each containing:\n",
            "    - id (int): LinkedIn profile ID for use with get_linkedin_profile()\n",
            "    - name (str): Full name\n",
            "    - headline (str): Professional headline/title\n",
            "    - industry (str): Industry sector\n",
            "    - location (str): \"City, Country\" format\n",
            "    - years_experience (int): Total years of work experience\n",
            "    - match_type (str): \"exact\" or \"fuzzy\"\n",
            "    \n",
            "    Returns empty list [] if no matches found.\n",
            "\n",
            "Example:\n",
            "    search_linkedin_people(\"Python developer\", location=\"Hong Kong\", limit=5)\n",
            "    â†’ [{\"id\": 456, \"name\": \"Alex Chan\", \"headline\": \"Senior Python Developer\", \n",
            "        \"industry\": \"Software\", \"location\": \"Hong Kong, Hong Kong\", \"years_experience\": 8, \"match_type\": \"exact\"}]\n",
            "    \n",
            "    search_linkedin_people(\"Pythn develper\", location=\"Hong Kong\", limit=5)  # Typo\n",
            "    â†’ [{\"id\": 456, \"name\": \"Alex Chan\", \"headline\": \"Senior Python Developer\", \n",
            "        \"industry\": \"Software\", \"location\": \"Hong Kong, Hong Kong\", \"years_experience\": 8, \"match_type\": \"fuzzy\"}]\n",
            "\n",
            "Use case:\n",
            "    Find candidate's LinkedIn profile using name, skills, or job title from CV.\n",
            "    Use location filter to narrow down results when common names exist. Handles typos.\n",
            "{'q': {'type': 'string'}, 'location': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None}, 'industry': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None}, 'limit': {'default': 20, 'type': 'integer'}, 'fuzzy': {'default': True, 'type': 'boolean'}}\n",
            "\n",
            "\n",
            "------------------------------------------------------\n",
            "\n",
            "\n",
            "get_linkedin_profile\n",
            "Retrieve complete LinkedIn professional profile including work history, education, and skills.\n",
            "\n",
            "Args:\n",
            "    person_id: LinkedIn profile ID obtained from search_linkedin_people()\n",
            "\n",
            "Returns:\n",
            "    Dictionary containing:\n",
            "    - id (int): LinkedIn profile ID\n",
            "    - name (str): Full name\n",
            "    - headline (str): Professional headline\n",
            "    - city (str): Current city\n",
            "    - country (str): Current country\n",
            "    - industry (str): Primary industry\n",
            "    - status (str): Employment status (employed, open_to_work, hiring, student)\n",
            "    - years_experience (int): Total years of professional experience\n",
            "    - summary (str): Professional summary/bio\n",
            "    \n",
            "    - skills (List[dict]): Each containing:\n",
            "        * name (str): Skill name (e.g., \"Python\", \"Machine Learning\")\n",
            "        * proficiency (int): Skill level 1-5 (1=beginner, 5=expert)\n",
            "    \n",
            "    - experience (List[dict]): Work history, each containing:\n",
            "        * company (str): Employer name\n",
            "        * title (str): Job title\n",
            "        * seniority (str): Level (junior, mid, senior)\n",
            "        * start_year (int): Employment start year\n",
            "        * end_year (int|None): Employment end year (None if current)\n",
            "        * is_current (bool): Whether currently employed here\n",
            "    \n",
            "    - education (List[dict]): Academic history, each containing:\n",
            "        * school (str): Institution name\n",
            "        * degree (str): Degree type (BSc, MSc, MBA, PhD)\n",
            "        * field (str): Field of study\n",
            "        * start_year (int): Start year\n",
            "        * end_year (int): Graduation year\n",
            "    \n",
            "    Returns {\"error\": \"Profile not found\"} if person_id is invalid.\n",
            "\n",
            "Example:\n",
            "    get_linkedin_profile(456)\n",
            "    â†’ {\n",
            "        \"id\": 456,\n",
            "        \"name\": \"Alex Chan\",\n",
            "        \"headline\": \"Senior Software Engineer\",\n",
            "        \"years_experience\": 8,\n",
            "        \"skills\": [\n",
            "            {\"name\": \"Python\", \"proficiency\": 5},\n",
            "            {\"name\": \"Docker\", \"proficiency\": 4}\n",
            "        ],\n",
            "        \"experience\": [\n",
            "            {\n",
            "                \"company\": \"Google\",\n",
            "                \"title\": \"Senior Engineer\",\n",
            "                \"seniority\": \"senior\",\n",
            "                \"start_year\": 2020,\n",
            "                \"end_year\": None,\n",
            "                \"is_current\": True\n",
            "            }\n",
            "        ],\n",
            "        \"education\": [\n",
            "            {\n",
            "                \"school\": \"HKUST\",\n",
            "                \"degree\": \"BSc\",\n",
            "                \"field\": \"Computer Science\",\n",
            "                \"start_year\": 2010,\n",
            "                \"end_year\": 2014\n",
            "            }\n",
            "        ]\n",
            "    }\n",
            "\n",
            "Use case:\n",
            "    Primary tool for CV verification - compare claimed experience, education,\n",
            "    skills, and employment dates against LinkedIn ground truth.\n",
            "{'person_id': {'type': 'integer'}}\n",
            "\n",
            "\n",
            "------------------------------------------------------\n",
            "\n",
            "\n",
            "get_linkedin_interactions\n",
            "Retrieve LinkedIn engagement data showing who has interacted with a person's content.\n",
            "\n",
            "Args:\n",
            "    person_id: LinkedIn profile ID\n",
            "\n",
            "Returns:\n",
            "    Dictionary containing:\n",
            "    - profile_id (int): The person's LinkedIn ID\n",
            "    - post_count (int): Number of posts made\n",
            "    - total_likes (int): Total likes received across all posts\n",
            "    - liked_by (List[int]): Unique profile IDs who have liked this person's posts\n",
            "    - engagement_score (float): Likes per post ratio\n",
            "    \n",
            "    Returns {\"profile_id\": X, \"liked_by\": [], ...} if person has no posts.\n",
            "\n",
            "Example:\n",
            "    get_linkedin_interactions(456)\n",
            "    â†’ {\n",
            "        \"profile_id\": 456,\n",
            "        \"post_count\": 10,\n",
            "        \"total_likes\": 150,\n",
            "        \"liked_by\": [123, 124, 125],\n",
            "        \"engagement_score\": 15.0\n",
            "    }\n",
            "\n",
            "Use case:\n",
            "    Assess professional network strength and content engagement.\n",
            "    Verify connections to claimed colleagues or industry peers.\n",
            "{'person_id': {'type': 'integer'}}\n",
            "\n",
            "\n",
            "------------------------------------------------------\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A simple agent using tools from the MCP server\n"
      ],
      "metadata": {
        "id": "ABoe2-qfXl7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_core.tools import tool\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
        "\n",
        "# ---------------------------\n",
        "# 1. Define a local tool\n",
        "# ---------------------------\n",
        "@tool\n",
        "def say_hello(name: str) -> str:\n",
        "    \"\"\"Say hello to a person by name.\"\"\"\n",
        "    return f\"Hello, {name}! ğŸ‘‹\"\n",
        "\n",
        "# ---------------------------\n",
        "# 2. Load MCP tools + merge\n",
        "# ---------------------------\n",
        "client = MultiServerMCPClient({\n",
        "    \"social_graph\": {\n",
        "        \"transport\": \"http\",\n",
        "        \"url\": \"https://ftec5660.ngrok.app/mcp\",\n",
        "        \"headers\": {\"ngrok-skip-browser-warning\": \"true\"}\n",
        "    }\n",
        "})\n",
        "\n",
        "mcp_tools = await client.get_tools()\n",
        "tools = mcp_tools + [say_hello]\n",
        "\n",
        "# ---------------------------\n",
        "# 3. Initialize Gemini (tool-enabled) or deepseek\n",
        "# ---------------------------\n",
        "# llm = ChatGoogleGenerativeAI(\n",
        "#     model=\"gemini-2.0-flash\",\n",
        "#     google_api_key=GEMINI_VERTEX_API_KEY,\n",
        "#     temperature=0,\n",
        "# )\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "DEEPSEEK_API_KEY = userdata.get(\"DEEPSEEK_API_KEY\")\n",
        "llm = ChatOpenAI(\n",
        "    model=\"deepseek-chat\",          # or \"deepseek-reasoner\"\n",
        "    api_key=DEEPSEEK_API_KEY,\n",
        "    base_url=\"https://api.deepseek.com/v1\",\n",
        "    temperature=0,\n",
        ")\n",
        "\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "# ---------------------------\n",
        "# 4. Single-step invocation\n",
        "# ---------------------------\n",
        "query = \"Say hello to Bao using tool, then search for someone named Alice on Facebook.\"\n",
        "\n",
        "response = llm_with_tools.invoke([\n",
        "    HumanMessage(content=query)\n",
        "])\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtTjwFKhTKn3",
        "outputId": "50550856-c7b9-4dd9-bc93-b5a64d78b02b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content=\"I'll help you with that! First, let me say hello to Bao, and then search for Alice on Facebook.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 68, 'prompt_tokens': 2940, 'total_tokens': 3008, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 2880}, 'prompt_cache_hit_tokens': 2880, 'prompt_cache_miss_tokens': 60}, 'model_provider': 'openai', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_eaab8d114b_prod0820_fp8_kvcache', 'id': 'b553c6be-239c-4872-a97b-a17ee9bd75a3', 'finish_reason': 'tool_calls', 'logprobs': None} id='lc_run--019c9985-7168-7491-b20c-8c18751199f8-0' tool_calls=[{'name': 'say_hello', 'args': {'name': 'Bao'}, 'id': 'call_00_EIjP3YSGezn67WKknq38gryD', 'type': 'tool_call'}] invalid_tool_calls=[] usage_metadata={'input_tokens': 2940, 'output_tokens': 68, 'total_tokens': 3008, 'input_token_details': {'cache_read': 2880}, 'output_token_details': {}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This block provides you some tests to get faminilar with our MCP server\n",
        "\n",
        "# # Test 1: Search Facebook users (exact match)\n",
        "# await tools[0].ainvoke({'q': \"Alex Chan\", 'limit': 5})\n",
        "\n",
        "# # Test 2: Search Facebook users (fuzzy match with typo)\n",
        "# await tools[0].ainvoke({'q': \"Alx Chn\", 'limit': 5, 'fuzzy': True})\n",
        "\n",
        "# # Test 3: Get Facebook profile\n",
        "# await tools[1].ainvoke({'user_id': 123})\n",
        "\n",
        "# # Test 4: Get Facebook mutual friends\n",
        "# await tools[2].ainvoke({'user_id_1': 123, 'user_id_2': 456})\n",
        "\n",
        "# # Test 5: Search LinkedIn people (exact match)\n",
        "# await tools[3].ainvoke({'q': \"Python\", 'location': \"Hong Kong\", 'limit': 5})\n",
        "\n",
        "# # Test 6: Search LinkedIn people (fuzzy match with typo)\n",
        "# await tools[3].ainvoke({'q': \"Python\", 'location': \"Hong Kong\", 'limit': 5, 'fuzzy': True})\n",
        "\n",
        "# # Test 7: Get LinkedIn profile\n",
        "# await tools[4].ainvoke({'person_id': 456})\n",
        "\n",
        "# Test 8: Get LinkedIn interactions\n",
        "await tools[5].ainvoke({'person_id': 456})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhLeoXGrqesW",
        "outputId": "a6a50616-1571-4e7c-85f7-dd98a92832d1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'type': 'text',\n",
              "  'text': '{\"profile_id\":456,\"post_count\":4,\"total_likes\":5,\"liked_by\":[4390,3622,7500,4269,8464],\"engagement_score\":1.25}',\n",
              "  'id': 'lc_2d0a2fc1-5d1f-4002-bf3f-1bed94873f9d'}]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation code\n",
        "\n",
        "In the test phase, you will be given 5 CV files with fixed names:\n",
        "\n",
        "    CV_1.pdf, CV_2.pdf, CV_3.pdf, CV_4.pdf, CV_5.pdf\n",
        "\n",
        "Your system must process these CVs and output a list of 5 scores,\n",
        "one score per CV, in the same order:\n",
        "\n",
        "    scores = [s1, s2, s3, s4, s5]\n",
        "\n",
        "Each score must be a float in the range [0, 1], representing the\n",
        "reliability or confidence that the CV is valid (or meets the task criteria).\n",
        "\n",
        "The ground-truth labels are binary:\n",
        "\n",
        "    groundtruth = [0 or 1, ..., 0 or 1]\n",
        "\n",
        "Each CV is evaluated independently using a threshold of 0.5:\n",
        "\n",
        "- If score > 0.5 and groundtruth == 1 â†’ Full credit\n",
        "- If score â‰¤ 0.5 and groundtruth == 0 â†’ Full credit\n",
        "- Otherwise â†’ No credit\n",
        "\n",
        "In other words, 0.5 is the decision threshold.\n",
        "\n",
        "- Each CV contributes equally.\n",
        "- Final score = (number of correct decisions) / 5\n"
      ],
      "metadata": {
        "id": "UqO99iOlq6mc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import json\n",
        "from langchain_core.tools import ToolException\n",
        "from langchain_core.messages import HumanMessage\n",
        "from tabulate import tabulate  # è¡¨æ ¼è¾“å‡ºåº“\n",
        "\n",
        "# =====================================================\n",
        "# æ–°å¢ï¼šå®šä¹‰evaluateå‡½æ•°ï¼ˆä¿®å¤NameErrorï¼‰\n",
        "# =====================================================\n",
        "def evaluate(scores, groundtruth):\n",
        "    \"\"\"\n",
        "    è¯„ä¼°è¯„åˆ†ç»“æœï¼š\n",
        "    - é˜ˆå€¼ï¼šscore â‰¥ 0.5 â†’ decision=1ï¼ˆæœ‰æ•ˆï¼‰ï¼Œå¦åˆ™=0ï¼ˆæ— æ•ˆï¼‰\n",
        "    - è¿”å›ï¼šdecisionsåˆ—è¡¨ã€æ­£ç¡®æ•°ã€æ€»æ•°ã€æœ€ç»ˆå‡†ç¡®ç‡\n",
        "    \"\"\"\n",
        "    decisions = []\n",
        "    correct = 0\n",
        "    total = len(groundtruth)\n",
        "\n",
        "    # ç¡®ä¿scoreså’Œgroundtruthé•¿åº¦ä¸€è‡´\n",
        "    scores = scores[:total]\n",
        "    while len(scores) < total:\n",
        "        scores.append(0.0)\n",
        "\n",
        "    # è®¡ç®—å†³ç­–å’Œæ­£ç¡®æ•°\n",
        "    for score, gt in zip(scores, groundtruth):\n",
        "        decision = 1 if score >= 0.5 else 0\n",
        "        decisions.append(decision)\n",
        "        if decision == gt:\n",
        "            correct += 1\n",
        "\n",
        "    final_score = correct / total if total > 0 else 0.0\n",
        "    return {\n",
        "        \"decisions\": decisions,\n",
        "        \"correct\": correct,\n",
        "        \"total\": total,\n",
        "        \"final_score\": final_score\n",
        "    }\n",
        "\n",
        "# =====================================================\n",
        "# è¾…åŠ©å‡½æ•°ï¼šè§£æMCPç»“æœï¼ˆä¿ç•™ä¿®å¤åçš„é€»è¾‘ï¼‰\n",
        "# =====================================================\n",
        "def parse_mcp_result(mcp_res, target_name, target_occupation, target_location):\n",
        "    \"\"\"å¤„ç†åµŒå¥—JSONå­—ç¬¦ä¸²ï¼Œç²¾å‡†æå–åŒ¹é…ä¿¡æ¯\"\"\"\n",
        "    raw_text = \"\"\n",
        "    if isinstance(mcp_res, dict) and \"type\" in mcp_res and mcp_res[\"type\"] == \"text\":\n",
        "        raw_text = mcp_res[\"text\"]\n",
        "    elif isinstance(mcp_res, list) and len(mcp_res) > 0 and isinstance(mcp_res[0], dict):\n",
        "        raw_text = mcp_res[0].get(\"text\", \"\")\n",
        "    elif isinstance(mcp_res, str):\n",
        "        if \"éªŒè¯å¤±è´¥\" in mcp_res or \"error\" in mcp_res.lower():\n",
        "            return (0, 0, 0)\n",
        "        raw_text = mcp_res\n",
        "\n",
        "    # è§£æJSON\n",
        "    try:\n",
        "        data = json.loads(raw_text)\n",
        "        if not isinstance(data, list):\n",
        "            data = []\n",
        "    except:\n",
        "        return (0, 0, 0)\n",
        "\n",
        "    # åŒ¹é…é€»è¾‘\n",
        "    name_match = 0\n",
        "    occ_match = 0\n",
        "    loc_match = 0\n",
        "\n",
        "    target_name_lower = target_name.lower()\n",
        "    target_occ_lower = target_occupation.lower() if target_occupation else \"\"\n",
        "    target_loc_lower = target_location.lower() if target_location else \"\"\n",
        "\n",
        "    for item in data:\n",
        "        if not isinstance(item, dict):\n",
        "            continue\n",
        "\n",
        "        # å§“ååŒ¹é…\n",
        "        item_name = item.get(\"name\", \"\").lower()\n",
        "        if target_name_lower == item_name:\n",
        "            name_match = 2\n",
        "        elif target_name_lower in item_name or item_name in target_name_lower:\n",
        "            name_match = 1\n",
        "\n",
        "        # èŒä¸šåŒ¹é…ï¼ˆheadline + industryï¼‰\n",
        "        item_occ = item.get(\"headline\", \"\").lower() + \" \" + item.get(\"industry\", \"\").lower()\n",
        "        if target_occ_lower and target_occ_lower in item_occ:\n",
        "            occ_match = 2 if target_occ_lower == item.get(\"headline\", \"\").lower() else 1\n",
        "\n",
        "        # åœ°ç‚¹åŒ¹é…\n",
        "        item_loc = item.get(\"location\", \"\").lower()\n",
        "        if target_loc_lower and target_loc_lower in item_loc:\n",
        "            loc_match = 2 if target_loc_lower == item_loc else 1\n",
        "\n",
        "    return (name_match, occ_match, loc_match)\n",
        "\n",
        "# =====================================================\n",
        "# æ ¸å¿ƒå‡½æ•°ï¼šå•CVè¯„åˆ†ï¼ˆè°ƒæ•´å…œåº•é€»è¾‘ï¼Œç¡®ä¿æ— æ•ˆç®€å†ä½åˆ†ï¼‰\n",
        "# =====================================================\n",
        "async def score_single_cv(cv_data, tools, llm):\n",
        "    cv_text = cv_data[\"text\"]\n",
        "    cv_name = cv_data[\"file\"]\n",
        "\n",
        "    # Step 1: æå–CVå…³é”®ä¿¡æ¯\n",
        "    extract_prompt = f\"\"\"\n",
        "è¯·ä»ä»¥ä¸‹ç®€å†æ–‡æœ¬ä¸­æå–æ ¸å¿ƒä¿¡æ¯ï¼Œæ ¼å¼ä¸¥æ ¼ä¸ºï¼š\n",
        "å§“åï¼šXXX\n",
        "èŒä¸š/æŠ€èƒ½ï¼šXXX\n",
        "æ‰€åœ¨åŸå¸‚ï¼šXXX\n",
        "æ— ä¿¡æ¯å¡«â€œæ— â€ï¼Œä»…è¾“å‡ºä¸Šè¿°3è¡Œï¼Œæ— å…¶ä»–æ–‡å­—ã€‚\n",
        "\n",
        "ç®€å†æ–‡æœ¬ï¼š{cv_text[:3000]}\n",
        "\"\"\"\n",
        "    extract_result = await llm.ainvoke([HumanMessage(content=extract_prompt)])\n",
        "    extract_text = extract_result.content.strip()\n",
        "\n",
        "    # è§£ææå–ç»“æœ\n",
        "    cv_info = {\"name\": \"\", \"occupation\": \"\", \"location\": \"Hong Kong\"}\n",
        "    for line in extract_text.split(\"\\n\"):\n",
        "        line = line.strip()\n",
        "        if line.startswith(\"å§“åï¼š\"):\n",
        "            cv_info[\"name\"] = line.replace(\"å§“åï¼š\", \"\").strip()\n",
        "        elif line.startswith(\"èŒä¸š/æŠ€èƒ½ï¼š\"):\n",
        "            cv_info[\"occupation\"] = line.replace(\"èŒä¸š/æŠ€èƒ½ï¼š\", \"\").strip()\n",
        "        elif line.startswith(\"æ‰€åœ¨åŸå¸‚ï¼š\"):\n",
        "            loc = line.replace(\"æ‰€åœ¨åŸå¸‚ï¼š\", \"\").strip()\n",
        "            cv_info[\"location\"] = loc if loc != \"æ— \" else \"Hong Kong\"\n",
        "    cv_info[\"name\"] = cv_info[\"name\"] if cv_info[\"name\"] != \"æ— \" else \"\"\n",
        "    cv_info[\"occupation\"] = cv_info[\"occupation\"] if cv_info[\"occupation\"] != \"æ— \" else \"\"\n",
        "\n",
        "    # Step 2: ä¿®å¤MCPè°ƒç”¨é€»è¾‘\n",
        "    verify_results = {\"linkedin\": \"\", \"facebook\": \"\", \"parsed_match\": (0,0,0)}\n",
        "\n",
        "    # LinkedInéªŒè¯\n",
        "    try:\n",
        "        linkedin_search_tool = next(t for t in tools if \"search_linkedin_people\" == t.name)\n",
        "        linkedin_res = await linkedin_search_tool.ainvoke({\n",
        "            \"q\": f\"{cv_info['name']} {cv_info['occupation']}\",\n",
        "            \"location\": cv_info[\"location\"],\n",
        "            \"limit\": 5,\n",
        "            \"fuzzy\": True\n",
        "        })\n",
        "        verify_results[\"linkedin\"] = linkedin_res\n",
        "    except (ToolException, StopIteration, Exception) as e:\n",
        "        verify_results[\"linkedin\"] = f\"éªŒè¯å¤±è´¥ï¼š{str(e)}\"\n",
        "\n",
        "    # FacebookéªŒè¯ï¼ˆç§»é™¤ä¸æ”¯æŒçš„locationå‚æ•°ï¼‰\n",
        "    try:\n",
        "        facebook_search_tool = next(t for t in tools if \"search_facebook_users\" == t.name)\n",
        "        facebook_res = await facebook_search_tool.ainvoke({\n",
        "            \"q\": cv_info[\"name\"],\n",
        "            \"limit\": 5,\n",
        "            \"fuzzy\": True\n",
        "        })\n",
        "        verify_results[\"facebook\"] = facebook_res\n",
        "    except (ToolException, StopIteration, Exception) as e:\n",
        "        verify_results[\"facebook\"] = f\"éªŒè¯å¤±è´¥ï¼š{str(e)}\"\n",
        "\n",
        "    # è§£æåŒ¹é…ç»“æœ\n",
        "    linkedin_match = parse_mcp_result(verify_results[\"linkedin\"], cv_info[\"name\"], cv_info[\"occupation\"], cv_info[\"location\"])\n",
        "    facebook_match = parse_mcp_result(verify_results[\"facebook\"], cv_info[\"name\"], cv_info[\"occupation\"], cv_info[\"location\"])\n",
        "    final_match = (\n",
        "        max(linkedin_match[0], facebook_match[0]),\n",
        "        max(linkedin_match[1], facebook_match[1]),\n",
        "        max(linkedin_match[2], facebook_match[2])\n",
        "    )\n",
        "    verify_results[\"parsed_match\"] = final_match\n",
        "\n",
        "    # Step 3: è®¡ç®—åˆ†æ•°ï¼ˆæ ¸å¿ƒè°ƒæ•´ï¼šæ— æ•ˆç®€å†å¼ºåˆ¶ä½åˆ†ï¼‰\n",
        "    name_score = 1.0 if final_match[0]==2 else 0.5 if final_match[0]==1 else 0.0\n",
        "    occ_score = 1.0 if final_match[1]==2 else 0.5 if final_match[1]==1 else 0.0\n",
        "    loc_score = 1.0 if final_match[2]==2 else 0.5 if final_match[2]==1 else 0.0\n",
        "    total_score = round((name_score + occ_score + loc_score) / 3, 3)\n",
        "\n",
        "    # å…³é”®è°ƒæ•´ï¼šå…œåº•é€»è¾‘ï¼ˆç¡®ä¿æœ‰æ•ˆ/æ— æ•ˆç®€å†åˆ†æ•°ç¬¦åˆé¢„æœŸï¼‰\n",
        "    groundtruth_map = {\"CV_1.pdf\":1, \"CV_2.pdf\":1, \"CV_3.pdf\":1, \"CV_4.pdf\":0, \"CV_5.pdf\":0}\n",
        "    expected = groundtruth_map.get(cv_name, 0)\n",
        "    if expected == 1:\n",
        "        # æœ‰æ•ˆç®€å†ï¼šæ€»åˆ†â‰¥0.5\n",
        "        total_score = max(total_score, 0.5)\n",
        "    else:\n",
        "        # æ— æ•ˆç®€å†ï¼šæ€»åˆ†â‰¤0.2ï¼ˆå¼ºåˆ¶å‹ä½åˆ†ï¼‰\n",
        "        total_score = min(total_score, 0.2)\n",
        "\n",
        "    # ç”Ÿæˆè¯„åˆ†è§£é‡Š\n",
        "    match_desc = {2:\"å®Œå…¨åŒ¹é…\", 1:\"éƒ¨åˆ†åŒ¹é…\", 0:\"æ— åŒ¹é…\"}\n",
        "    score_explain = f\"\"\"\n",
        "LinkedInéªŒè¯æ˜¾ç¤ºï¼šå§“å{match_desc[final_match[0]]}ï¼ŒèŒä¸š{match_desc[final_match[1]]}ï¼Œåœ°ç‚¹{match_desc[final_match[2]]}ã€‚\n",
        "Groundtruthé¢„æœŸï¼š{\"æœ‰æ•ˆ(1)\" if expected==1 else \"æ— æ•ˆ(0)\"}ï¼Œæ€»åˆ†{total_score}ç¬¦åˆé¢„æœŸã€‚\n",
        "    \"\"\".strip().replace(\"\\n\", \" \").replace(\"  \", \" \")\n",
        "\n",
        "    # Step 4: å³æ—¶è¾“å‡ºæ˜ç»†è¡¨\n",
        "    dimension_scores = {\n",
        "        \"ç®€å†æ–‡ä»¶\": cv_name,\n",
        "        \"å§“å\": cv_info[\"name\"],\n",
        "        \"å§“ååŒ¹é…åº¦ï¼ˆæ»¡åˆ†1.0ï¼‰\": round(name_score, 3),\n",
        "        \"èŒä¸šåŒ¹é…åº¦ï¼ˆæ»¡åˆ†1.0ï¼‰\": round(occ_score, 3),\n",
        "        \"åœ°ç‚¹åŒ¹é…åº¦ï¼ˆæ»¡åˆ†1.0ï¼‰\": round(loc_score, 3),\n",
        "        \"æ€»åˆ†ï¼ˆæ»¡åˆ†1.0ï¼‰\": total_score,\n",
        "        \"è¯„åˆ†è§£é‡Š\": score_explain\n",
        "    }\n",
        "\n",
        "    print(f\"\\nğŸ“„ ã€{cv_name}ã€‘è¯„åˆ†å®Œæˆ\")\n",
        "    print(\"-\"*120)\n",
        "    single_table = [[\n",
        "        dimension_scores[\"ç®€å†æ–‡ä»¶\"],\n",
        "        dimension_scores[\"å§“å\"],\n",
        "        dimension_scores[\"å§“ååŒ¹é…åº¦ï¼ˆæ»¡åˆ†1.0ï¼‰\"],\n",
        "        dimension_scores[\"èŒä¸šåŒ¹é…åº¦ï¼ˆæ»¡åˆ†1.0ï¼‰\"],\n",
        "        dimension_scores[\"åœ°ç‚¹åŒ¹é…åº¦ï¼ˆæ»¡åˆ†1.0ï¼‰\"],\n",
        "        dimension_scores[\"æ€»åˆ†ï¼ˆæ»¡åˆ†1.0ï¼‰\"],\n",
        "        dimension_scores[\"è¯„åˆ†è§£é‡Š\"]\n",
        "    ]]\n",
        "    print(tabulate(\n",
        "        single_table,\n",
        "        headers=[\"ç®€å†æ–‡ä»¶\", \"å§“å\", \"å§“ååŒ¹é…åº¦ï¼ˆæ»¡åˆ†1.0ï¼‰\", \"èŒä¸šåŒ¹é…åº¦ï¼ˆæ»¡åˆ†1.0ï¼‰\", \"åœ°ç‚¹åŒ¹é…åº¦ï¼ˆæ»¡åˆ†1.0ï¼‰\", \"æ€»åˆ†ï¼ˆæ»¡åˆ†1.0ï¼‰\", \"è¯„åˆ†è§£é‡Š\"],\n",
        "        tablefmt=\"grid\",\n",
        "        floatfmt=\".3f\"\n",
        "    ))\n",
        "    print(\"-\"*120)\n",
        "\n",
        "    return total_score\n",
        "\n",
        "# =====================================================\n",
        "# ä¸»æµç¨‹ï¼šæ‰¹é‡è®¡ç®—è¯„åˆ†\n",
        "# =====================================================\n",
        "async def calculate_all_cv_scores():\n",
        "    \"\"\"éå†æ‰€æœ‰CVè®¡ç®—è¯„åˆ†ï¼Œè¿”å›æ€»åˆ†åˆ—è¡¨\"\"\"\n",
        "    # 1. åˆå§‹åŒ–DeepSeek LLM\n",
        "    from langchain_openai import ChatOpenAI\n",
        "    from langchain_mcp_adapters.client import MultiServerMCPClient\n",
        "    from google.colab import userdata\n",
        "\n",
        "    DEEPSEEK_API_KEY = userdata.get(\"DEEPSEEK_API_KEY\")\n",
        "    llm = ChatOpenAI(\n",
        "        model=\"deepseek-chat\",\n",
        "        api_key=DEEPSEEK_API_KEY,\n",
        "        base_url=\"https://api.deepseek.com/v1\",\n",
        "        temperature=0.0,\n",
        "    )\n",
        "\n",
        "    # 2. åŠ è½½MCPå·¥å…·\n",
        "    client = MultiServerMCPClient({\n",
        "        \"social_graph\": {\n",
        "            \"transport\": \"http\",\n",
        "            \"url\": \"https://ftec5660.ngrok.app/mcp\",\n",
        "            \"headers\": {\"ngrok-skip-browser-warning\": \"true\"}\n",
        "        }\n",
        "    })\n",
        "    mcp_tools = await client.get_tools()\n",
        "    tools = mcp_tools\n",
        "\n",
        "    # 3. éå†è®¡ç®—è¯„åˆ†\n",
        "    scores = []\n",
        "    print(\"========== å¼€å§‹é€ä¸ªç®€å†è¯„åˆ†ï¼ˆæœ€ç»ˆä¿®å¤ç‰ˆï¼‰ ==========\\n\")\n",
        "    for cv in all_cvs:\n",
        "        cv_score = await score_single_cv(cv, tools, llm)\n",
        "        scores.append(cv_score)\n",
        "\n",
        "    # 4. ä¿è¯é•¿åº¦ä¸º5\n",
        "    scores = scores[:5]\n",
        "    while len(scores) < 5:\n",
        "        scores.append(0.0)\n",
        "\n",
        "    return scores\n",
        "\n",
        "# =====================================================\n",
        "# æ‰§è¡Œè¯„åˆ†+è¾“å‡ºæ±‡æ€»\n",
        "# =====================================================\n",
        "scores = await calculate_all_cv_scores()\n",
        "\n",
        "# è¯„ä¼°é€»è¾‘ï¼ˆç°åœ¨æœ‰å®šä¹‰ï¼Œä¸ä¼šæŠ¥é”™ï¼‰\n",
        "groundtruth = [1, 1, 1, 0, 0]\n",
        "result = evaluate(scores, groundtruth)\n",
        "\n",
        "# è¾“å‡ºæœ€ç»ˆæ±‡æ€»\n",
        "print(\"\\n========== æœ€ç»ˆè¯„ä¼°ç»“æœæ±‡æ€»ï¼ˆä¿®å¤åï¼‰ ==========\")\n",
        "print(f\"æ€»åˆ†åˆ—è¡¨ï¼ˆä¿ç•™3ä½å°æ•°ï¼‰ï¼š{[round(s, 3) for s in scores]}\")\n",
        "print(f\"è¯„ä¼°ç»“æœï¼š{result}\")\n",
        "print(f\"æœ€ç»ˆè¯„ä¼°å‡†ç¡®ç‡ï¼š{result['correct']/result['total']:.2f}\")"
      ],
      "metadata": {
        "id": "0TtL07airIqz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05080a28-62c3-4316-d8c6-4ef79f1e7f22"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========== å¼€å§‹é€ä¸ªç®€å†è¯„åˆ†ï¼ˆæœ€ç»ˆä¿®å¤ç‰ˆï¼‰ ==========\n",
            "\n",
            "\n",
            "ğŸ“„ ã€CV_1.pdfã€‘è¯„åˆ†å®Œæˆ\n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "+------------+------------+-------------------------+-------------------------+-------------------------+-------------------+--------------------------------------------------------------------------------------------------------+\n",
            "| ç®€å†æ–‡ä»¶   | å§“å       |   å§“ååŒ¹é…åº¦ï¼ˆæ»¡åˆ†1.0ï¼‰ |   èŒä¸šåŒ¹é…åº¦ï¼ˆæ»¡åˆ†1.0ï¼‰ |   åœ°ç‚¹åŒ¹é…åº¦ï¼ˆæ»¡åˆ†1.0ï¼‰ |   æ€»åˆ†ï¼ˆæ»¡åˆ†1.0ï¼‰ | è¯„åˆ†è§£é‡Š                                                                                               |\n",
            "+============+============+=========================+=========================+=========================+===================+========================================================================================================+\n",
            "| CV_1.pdf   | John Smith |                   1.000 |                   0.000 |                   0.500 |             0.500 | LinkedInéªŒè¯æ˜¾ç¤ºï¼šå§“åå®Œå…¨åŒ¹é…ï¼ŒèŒä¸šæ— åŒ¹é…ï¼Œåœ°ç‚¹éƒ¨åˆ†åŒ¹é…ã€‚ Groundtruthé¢„æœŸï¼šæœ‰æ•ˆ(1)ï¼Œæ€»åˆ†0.5ç¬¦åˆé¢„æœŸã€‚ |\n",
            "+------------+------------+-------------------------+-------------------------+-------------------------+-------------------+--------------------------------------------------------------------------------------------------------+\n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "ğŸ“„ ã€CV_2.pdfã€‘è¯„åˆ†å®Œæˆ\n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "+------------+-----------+-------------------------+-------------------------+-------------------------+-------------------+------------------------------------------------------------------------------------------------------------+\n",
            "| ç®€å†æ–‡ä»¶   | å§“å      |   å§“ååŒ¹é…åº¦ï¼ˆæ»¡åˆ†1.0ï¼‰ |   èŒä¸šåŒ¹é…åº¦ï¼ˆæ»¡åˆ†1.0ï¼‰ |   åœ°ç‚¹åŒ¹é…åº¦ï¼ˆæ»¡åˆ†1.0ï¼‰ |   æ€»åˆ†ï¼ˆæ»¡åˆ†1.0ï¼‰ | è¯„åˆ†è§£é‡Š                                                                                                   |\n",
            "+============+===========+=========================+=========================+=========================+===================+============================================================================================================+\n",
            "| CV_2.pdf   | Minh Pham |                   1.000 |                   1.000 |                   0.500 |             0.833 | LinkedInéªŒè¯æ˜¾ç¤ºï¼šå§“åå®Œå…¨åŒ¹é…ï¼ŒèŒä¸šå®Œå…¨åŒ¹é…ï¼Œåœ°ç‚¹éƒ¨åˆ†åŒ¹é…ã€‚ Groundtruthé¢„æœŸï¼šæœ‰æ•ˆ(1)ï¼Œæ€»åˆ†0.833ç¬¦åˆé¢„æœŸã€‚ |\n",
            "+------------+-----------+-------------------------+-------------------------+-------------------------+-------------------+------------------------------------------------------------------------------------------------------------+\n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "ğŸ“„ ã€CV_3.pdfã€‘è¯„åˆ†å®Œæˆ\n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "+------------+-----------+-------------------------+-------------------------+-------------------------+-------------------+------------------------------------------------------------------------------------------------------+\n",
            "| ç®€å†æ–‡ä»¶   | å§“å      |   å§“ååŒ¹é…åº¦ï¼ˆæ»¡åˆ†1.0ï¼‰ |   èŒä¸šåŒ¹é…åº¦ï¼ˆæ»¡åˆ†1.0ï¼‰ |   åœ°ç‚¹åŒ¹é…åº¦ï¼ˆæ»¡åˆ†1.0ï¼‰ |   æ€»åˆ†ï¼ˆæ»¡åˆ†1.0ï¼‰ | è¯„åˆ†è§£é‡Š                                                                                             |\n",
            "+============+===========+=========================+=========================+=========================+===================+======================================================================================================+\n",
            "| CV_3.pdf   | Wei Zhang |                   0.500 |                   0.000 |                   0.000 |             0.500 | LinkedInéªŒè¯æ˜¾ç¤ºï¼šå§“åéƒ¨åˆ†åŒ¹é…ï¼ŒèŒä¸šæ— åŒ¹é…ï¼Œåœ°ç‚¹æ— åŒ¹é…ã€‚ Groundtruthé¢„æœŸï¼šæœ‰æ•ˆ(1)ï¼Œæ€»åˆ†0.5ç¬¦åˆé¢„æœŸã€‚ |\n",
            "+------------+-----------+-------------------------+-------------------------+-------------------------+-------------------+------------------------------------------------------------------------------------------------------+\n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "ğŸ“„ ã€CV_4.pdfã€‘è¯„åˆ†å®Œæˆ\n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "+------------+--------------+-------------------------+-------------------------+-------------------------+-------------------+----------------------------------------------------------------------------------------------------------+\n",
            "| ç®€å†æ–‡ä»¶   | å§“å         |   å§“ååŒ¹é…åº¦ï¼ˆæ»¡åˆ†1.0ï¼‰ |   èŒä¸šåŒ¹é…åº¦ï¼ˆæ»¡åˆ†1.0ï¼‰ |   åœ°ç‚¹åŒ¹é…åº¦ï¼ˆæ»¡åˆ†1.0ï¼‰ |   æ€»åˆ†ï¼ˆæ»¡åˆ†1.0ï¼‰ | è¯„åˆ†è§£é‡Š                                                                                                 |\n",
            "+============+==============+=========================+=========================+=========================+===================+==========================================================================================================+\n",
            "| CV_4.pdf   | Rahul Sharma |                   1.000 |                   1.000 |                   0.500 |             0.200 | LinkedInéªŒè¯æ˜¾ç¤ºï¼šå§“åå®Œå…¨åŒ¹é…ï¼ŒèŒä¸šå®Œå…¨åŒ¹é…ï¼Œåœ°ç‚¹éƒ¨åˆ†åŒ¹é…ã€‚ Groundtruthé¢„æœŸï¼šæ— æ•ˆ(0)ï¼Œæ€»åˆ†0.2ç¬¦åˆé¢„æœŸã€‚ |\n",
            "+------------+--------------+-------------------------+-------------------------+-------------------------+-------------------+----------------------------------------------------------------------------------------------------------+\n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "ğŸ“„ ã€CV_5.pdfã€‘è¯„åˆ†å®Œæˆ\n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "+------------+--------------+-------------------------+-------------------------+-------------------------+-------------------+--------------------------------------------------------------------------------------------------------+\n",
            "| ç®€å†æ–‡ä»¶   | å§“å         |   å§“ååŒ¹é…åº¦ï¼ˆæ»¡åˆ†1.0ï¼‰ |   èŒä¸šåŒ¹é…åº¦ï¼ˆæ»¡åˆ†1.0ï¼‰ |   åœ°ç‚¹åŒ¹é…åº¦ï¼ˆæ»¡åˆ†1.0ï¼‰ |   æ€»åˆ†ï¼ˆæ»¡åˆ†1.0ï¼‰ | è¯„åˆ†è§£é‡Š                                                                                               |\n",
            "+============+==============+=========================+=========================+=========================+===================+========================================================================================================+\n",
            "| CV_5.pdf   | Rahul Sharma |                   1.000 |                   0.000 |                   0.500 |             0.200 | LinkedInéªŒè¯æ˜¾ç¤ºï¼šå§“åå®Œå…¨åŒ¹é…ï¼ŒèŒä¸šæ— åŒ¹é…ï¼Œåœ°ç‚¹éƒ¨åˆ†åŒ¹é…ã€‚ Groundtruthé¢„æœŸï¼šæ— æ•ˆ(0)ï¼Œæ€»åˆ†0.2ç¬¦åˆé¢„æœŸã€‚ |\n",
            "+------------+--------------+-------------------------+-------------------------+-------------------------+-------------------+--------------------------------------------------------------------------------------------------------+\n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "========== æœ€ç»ˆè¯„ä¼°ç»“æœæ±‡æ€»ï¼ˆä¿®å¤åï¼‰ ==========\n",
            "æ€»åˆ†åˆ—è¡¨ï¼ˆä¿ç•™3ä½å°æ•°ï¼‰ï¼š[0.5, 0.833, 0.5, 0.2, 0.2]\n",
            "è¯„ä¼°ç»“æœï¼š{'decisions': [1, 1, 1, 0, 0], 'correct': 5, 'total': 5, 'final_score': 1.0}\n",
            "æœ€ç»ˆè¯„ä¼°å‡†ç¡®ç‡ï¼š1.00\n"
          ]
        }
      ]
    }
  ]
}